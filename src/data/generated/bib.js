const generatedBibEntries = {
    "Beck2016Visual": {
        "abstract": "We present AIST++, a new multi-modal dataset of 3D dance motion and music, along with FACT, a Full-Attention Cross-modal Transformer network for generating 3D dance motion conditioned on music. The proposed AIST++ dataset contains 5.2 hours of 3D dance motion in 1408 sequences, covering 10 dance genres with multi-view videos with known camera poses\u2014the largest dataset of this kind to our knowledge. We show that naively applying sequence models such as transformers to this dataset for the task of music conditioned 3D motion generation does not produce satisfactory 3D motion that is well correlated with the input music. We overcome these shortcomings by introducing key changes in its architecture design and supervision: FACT model involves a deep cross-modal transformer block with full-attention that is trained to predict N future motions. We empirically show that these changes are key factors in generating long sequences of realistic dance motion that are well-attuned to the input music. We conduct extensive experiments on AIST++ with user studies, where our method outperforms recent state-of-the-art methods both qualitatively and quantitatively.",
        "author": "Ruilong Li and Shan Yang and David A. Ross and Angjoo Kanazawa",
        "doi": "10.1109/ICCV48922.2021.01315",
        "journal": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)",
        "keywords": "3D dance generation, cross-modal transformer, AIST++ dataset",
        "number": "01",
        "publisher": "IEEE",
        "series": "",
        "title": "AI Choreographer:  Music Conditioned 3D Dance Generation with AIST plus plus",
        "type": "article",
        "url": "https://arxiv.org/abs/2101.08779",
        "volume": "22",
        "year": "2021"
    },
    "Chen2025XDancer": {
        "abstract": "We propose X-Dancer, a novel framework for generating expressive human dance videos conditioned on music. Our method captures the nuances of musical expression and translates them into realistic and synchronized dance movements, enhancing the creative process of dance video production.",
        "author": "Zeyuan Chen and Hongyi Xu and Guoxian Song and You Xie and Chenxu Zhang and Xin Chen and Chao Wang and Di Chang and Linjie Luo",
        "doi": "10.48550/arXiv.2502.17414",
        "journal": "arXiv preprint arXiv:2502.17414",
        "keywords": "dance video generation, music-conditioned motion, expressive choreography",
        "number": "01",
        "publisher": "arXiv",
        "series": "",
        "title": "X-Dancer: Expressive Music to Human Dance Video Generation",
        "type": "article",
        "url": "https://arxiv.org/abs/2502.17414",
        "volume": "2025",
        "year": "2025"
    },
    "Dong2025EveryImage": {
        "abstract": "We present a novel approach to animate static images in a music-driven manner. Our method leverages advanced generative models to create realistic and expressive animations that synchronize with the input music, enabling a new form of multimedia content creation.",
        "author": "Zhikang Dong and Weituo Hao and Ju-Chiang Wang and Peng Zhang and Pawel Polak",
        "doi": "10.48550/arXiv.2501.18801",
        "journal": "arXiv preprint arXiv:2501.18801",
        "keywords": "image animation, music-driven generation, generative models",
        "number": "01",
        "publisher": "arXiv",
        "series": "",
        "title": "Every Image Listens, Every Image Dances: Music-Driven Image Animation",
        "type": "article",
        "url": "https://arxiv.org/abs/2501.18801",
        "volume": "2025",
        "year": "2025"
    },
    "Gong2023TM2D": {
        "abstract": "We propose a new task that simultaneously utilizes both music and text for 3D dance generation. Different from the existing works that generate dance through a single modality such as music, we hope that the instructive information provided by text can guide humans to perform richer movements while dancing. However, the existing datasets only contain paired motion with a single modality, e.g., music-dance or text-motion. Tackling this challenge, we utilize a 3D human motion VQ-VAE to project the motions of the two datasets into a latent space that consists of a series of quantized vectors so that the motion token from two datasets with different distributions can be effectively mixed for training. Furthermore, we propose a cross modal transformer architecture to integrate text instructions to generate the 3D dance without degrading the performance of music-conditioned dance generation. To better evaluate the quality of the generated motion, we introduce two metrics, MDP and Freezing score, to measure the coherence and freezing percentage of the generated motion. Extensive experiments show that our approach can generate realistic and coherent dance motion conditioned on both music and text while keeping the comparable performance conditioned on two single modalities (i.e., music2dance, text2motion).",
        "author": "Kehong Gong and Dongze Lian and Heng Chang and Chuan Guo and Zihang Jiang and Xinxin Zuo and Michael Bi Mi and Xinchao Wang",
        "booktitle": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)",
        "keywords": "3D dance generation, music-text integration, cross-modal transformer",
        "month": "October",
        "pages": "9942--9952",
        "publisher": "IEEE",
        "series": "",
        "title": "TM2D: Bimodality Driven 3D Dance Generation via Music-Text Integration",
        "type": "inproceedings",
        "url": "https://garfield-kh.github.io/TM2D/",
        "volume": "2023",
        "year": "2023"
    },
    "Kim2022Music2Video": {
        "abstract": "Creation of images using generative adversarial networks has been widely adapted into multi-modal regimes with the advent of multi-modal representation models pre-trained widely adapted with generative models. Since CLIP is a pre-trained model that can measure similarity between image and text in a shared latent space, a variety of tasks that employ both image and text rely on the representational power of CLIP. In this paper, we propose a framework for generating music videos based on the fusion of text and audio, leveraging the capabilities of CLIP and generative models.",
        "author": "Yoonjeon Kim and Joel Jang and Sumin Shin",
        "doi": "10.48550/arXiv.2201.03809",
        "journal": "arXiv preprint arXiv:2201.03809",
        "keywords": "music video generation, audio-text fusion, generative models",
        "number": "01",
        "publisher": "arXiv",
        "series": "",
        "title": "Music2Video: Automatic Generation of Music Video with Fusion of Audio and Text",
        "type": "article",
        "url": "https://arxiv.org/abs/2201.03809",
        "volume": "2022",
        "year": "2022"
    },
    "Li2021Music2Dance": {
        "abstract": "In this article, we propose a novel autoregressive generative model, DanceNet, to take the style, rhythm, and melody of music as the control signals to generate dance movements. DanceNet is composed of a music encoder and a dance decoder. The music encoder extracts music features, and the dance decoder generates corresponding dance movements. Extensive experiments demonstrate that our model can generate realistic and diverse dance movements that align well with the input music.",
        "author": "Yinghao Li and Zhen Zhu and Junting Zhang and Yebin Liu",
        "doi": "10.1145/3485664",
        "journal": "ACM Transactions on Graphics (TOG)",
        "keywords": "music-driven dance generation, autoregressive model, DanceNet",
        "number": "6",
        "publisher": "ACM",
        "series": "",
        "title": "Music2Dance: DanceNet for Music-Driven Dance Generation",
        "type": "article",
        "url": "https://dl.acm.org/doi/10.1145/3485664",
        "volume": "40",
        "year": "2021"
    },
    "Marchellus2023M2C": {
        "abstract": "We introduce music codes, a novel music representation for generating music-controlled dance. Moreover, we propose the M2C to formulate music codes and the SM-GPT network to predict dance motions using these codes.",
        "author": "Matthew Marchellus and In Kyu Park",
        "doi": "",
        "journal": "Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
        "keywords": "music representation, 3D dance generation, SM-GPT network",
        "number": "01",
        "publisher": "IEEE",
        "series": "",
        "title": "M2C: Concise Music Representation for 3D Dance Generation",
        "type": "article",
        "url": "https://openaccess.thecvf.com/content/ICCV2023W/AMFG/html/Marchellus_M2C_Concise_Music_Representation_for_3D_Dance_Generation_ICCVW_2023_paper.html",
        "volume": "2023",
        "year": "2023"
    },
    "Ren2019MusicOriented": {
        "abstract": "We present a learning-based approach with pose perceptual loss for automatic music video generation. Our method can produce a realistic dance video that conforms to the beats and rhymes of almost any given music. To achieve this, we firstly generate a human skeleton sequence from music and then apply the learned pose-to-appearance mapping to generate the final video. In the stage of generating skeleton sequences, we utilize two discriminators to capture different aspects of the sequence and propose a novel pose perceptual loss to produce natural dances. Besides, we also provide a new cross-modal evaluation to evaluate the dance quality, which is able to estimate the similarity between two modalities of music and dance. Finally, a user study is conducted to demonstrate that dance video synthesized by the presented approach produces surprisingly realistic results.",
        "author": "Xuanchi Ren and Haoran Li and Zijian Huang and Qifeng Chen",
        "doi": "10.48550/arXiv.1912.06606",
        "journal": "arXiv preprint arXiv:1912.06606",
        "keywords": "music video generation, pose perceptual loss, cross-modal evaluation",
        "number": "01",
        "publisher": "arXiv",
        "series": "",
        "title": "Music-oriented Dance Video Synthesis with Pose Perceptual Loss",
        "type": "article",
        "url": "https://arxiv.org/abs/1912.06606",
        "volume": "2019",
        "year": "2019"
    },
    "Tseng2022EDGE": {
        "abstract": "Dance is an important human art form, but creating new dances can be difficult and time-consuming. In this work, we introduce Editable Dance Generation (EDGE), a state-of-the-art method for editable dance generation that is capable of creating realistic, physically-plausible dances while remaining faithful to the input music.",
        "author": "Jonathan Tseng and Rodrigo Castellon and C. Karen Liu",
        "doi": "10.48550/arXiv.2211.10658",
        "journal": "arXiv preprint arXiv:2211.10658",
        "keywords": "dance generation, editable choreography, music-conditioned motion",
        "number": "01",
        "publisher": "arXiv",
        "series": "",
        "title": "EDGE: Editable Dance Generation From Music",
        "type": "article",
        "url": "https://arxiv.org/abs/2211.10658",
        "volume": "2022",
        "year": "2022"
    },
    "Zhang2025MotionAnything": {
        "abstract": "We introduce Motion Anything, a versatile framework capable of generating motion sequences from various input modalities. Our approach demonstrates high-quality motion generation across diverse scenarios, showcasing its adaptability and effectiveness.",
        "author": "Zeyu Zhang and Yiran Wang and Wei Mao and Danning Li and Rui Zhao and Biao Wu and Zirui Song and Bohan Zhuang and Ian Reid and Richard Hartley",
        "doi": "10.48550/arXiv.2503.06955",
        "journal": "arXiv preprint arXiv:2503.06955",
        "keywords": "motion generation, multi-modal input, generative models",
        "number": "01",
        "publisher": "arXiv",
        "series": "",
        "title": "Motion Anything: Any to Motion Generation",
        "type": "article",
        "url": "https://arxiv.org/abs/2503.06955",
        "volume": "2025",
        "year": "2025"
    }
};